{"cells":[{"cell_type":"markdown","metadata":{"id":"XBITN0M_LKds"},"source":["# HW2P2: Face Classification and Verification\n"]},{"cell_type":"markdown","metadata":{"id":"-NH4P-HzLRQs"},"source":["Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early!\n","\n","\n","*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7001 identities\n","*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities"]},{"cell_type":"markdown","metadata":{"id":"i1B_m84_cU6c"},"source":["Common errors which you may face in this homeworks (because of the size of the model)\n","\n","\n","*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### To run this notebook, follow the following instructions.\n","* Kaggle notebook was used for this homework.\n","* Add competition dataset to you notebook\n","* Make sure the paths to the dataset are correct\n","* Run each cell in a chronological order until you submit on Kaggle for classification task.\n","* Note the change in wandb run for logging finetuning traing process.\n","* Load the best model for fine-tuning\n","* Run the remaining part of the code to fine-tune the model for verification task.\n","* After fine-tuning the model, the best model is loaded and should be used for prediction for both classification and verification tasks.\n","### Some of the architectures tried\n","#### CNN architecture with 512, 1024, 512, 512 layers and a single linear layer classifiers with Relu activation after each layer\n","* The transformations used were ToTensor(), RandomHorizontalFlip(p=0.5), and RandomRotation(degrees=25)\n","* SDG optimizer with (momentum=0.9, weight_decay=1e-4), ReduceLROnPlateau learning rate scheduler\n","* learning rate = 0.1\n","* Batchsize 64\n","* 50 epochs\n","* This obtained the train acc =99.98, and val acc = 72.6\n","\n","#### 3-layer Resnet (medium cut-off)\n","* transformations: RandomPerspective(0.3, 0.3), ColorJitter(brightness=0.2, contrast=0.2, saturation=0, hue=0), RandomRotation(degrees=25),RandomHorizontalFlip(p=0.5), RandAugment(4), ToTensor()\n","* SDG optimizer with (momentum=0.9, weight_decay=1e-4), ReduceLROnPlateau learning rate scheduler\n","* learning rate = 0.1\n","* Batchsize = 128\n","* epochs = 134 \n","* This obtained the train acc = 98.98, and val acc = 86.48\n","\n","#### SE-Resnet (High cut-off classification)\n","* [4, 5, 6, 2] layers\n","* transformations: RandomPerspective(0.3, 0.3), ColorJitter(brightness=0.2, contrast=0.2, saturation=0, hue=0), RandomRotation(degrees=25),RandomHorizontalFlip(p=0.5), RandAugment(4), ToTensor()\n","* SDG optimizer with (momentum=0.9, weight_decay=1e-4), ReduceLROnPlateau learning rate scheduler\n","* learning rate = 0.1\n","* 0.2 label smoothing\n","* Batchsize = 128\n","* 217 epochs\n","* This obtained the train acc = 99.929, and val acc = 90.422\n","* The model has 20,655,601 parameters\\\n","\n","Then, this SE-Resnet model was finetuned for verification task using centerloss.\n","* The model was retrained using 2 optimizers and losses \n","* The learning rate for SGD optimizer was fixed to 0.0001\n","* The learning rate for center loss optimizer was initialized to 0.1\n","* Center loss learning rate scheduler is CosineAnnealingLR\n","* loss weight = 0.0025\n","* batchsize = 64\n","* 97 epochs\n","* It obtained verification accuracy of 56.597 and classification accuracy of 90.7 on Kaggle public score.\n","\n","Additional details on training logs are found on wandb link provided."]},{"cell_type":"markdown","metadata":{"id":"BdoDIKWOMF59"},"source":["# Preliminaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jza7lwiScUhb","outputId":"c2846eb5-7aaa-4448-bc7e-700ec0f81bad","trusted":true},"outputs":[],"source":["!nvidia-smi # to see what GPU you have"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTxfd_nqFnL9","outputId":"6a5c6f16-8fec-4755-abad-8153aac2cf3a","trusted":true},"outputs":[],"source":["!pip install wandb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade wandb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwLEd0gdPbSc","outputId":"bd8f7edf-c0ea-44b9-c7e4-93f0fd5bbf85","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision #This library is used for image-based operations (Augmentations)\n","import os\n","import gc\n","from tqdm import tqdm\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","import glob\n","import io\n","import wandb\n","import matplotlib.pyplot as plt\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"1oxQNl-YVWHc"},"source":["# TODOs\n","As you go, please read the code and keep an eye out for TODOs!"]},{"cell_type":"markdown","metadata":{"id":"scOnMklwWBY6"},"source":["# Download Data from Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BksgPdkQwwb","outputId":"f0cd3d14-f0b9-4c58-d383-168420c51c3f","trusted":true},"outputs":[],"source":["!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"gbwiraye\",\"key\":\"<key>\"}')\n","    # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"markdown","metadata":{"id":"O68hT27SXClj"},"source":["# Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7qpMxG0XCJz","trusted":true},"outputs":[],"source":["config = {\n","    'batch_size': 64,  # Adjust based on GPU memory, can increase if GPU allows.\n","    'lr': 0.1,         # Learning rate for the optimizer\n","    'epochs': 217,      # Number of training epochs\n","    'rotation_angle': 25,\n","    'horizontal_flip': 0.5,\n","\n","}\n","\n","# You should expand this configuration further based on your specific requirements and dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"sSeiKHYrM-6b"},"source":["# Classification Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmRX5omaNDEZ","outputId":"a0b55a39-9ad9-4081-9230-bc5ffc3c5235","trusted":true},"outputs":[],"source":["DATA_DIR    = '/kaggle/input/11-785-f23-hw2p2-classification/11-785-f23-hw2p2-classification'# TODO: Path where you have downloaded the data\n","TRAIN_DIR   = os.path.join(DATA_DIR, \"train\")\n","VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n","TEST_DIR    = os.path.join(DATA_DIR, \"test\")\n","\n","\n","\n","# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n","\n","train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.RandomPerspective(0.3, 0.3),\n","    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0, hue=0),\n","    torchvision.transforms.RandomRotation(degrees=config['rotation_angle']),\n","    torchvision.transforms.RandomHorizontalFlip(p=config['horizontal_flip']),\n","    torchvision.transforms.RandAugment(4),\n","    torchvision.transforms.ToTensor()\n","])# Implementing the right train transforms/augmentation methods is key to improving performance.\n","\n","# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n","# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n","# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n","\n","valid_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor()\n","])\n","\n","\n","train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms)\n","valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform= valid_transforms)\n","# You should NOT have data augmentation on the validation set. Why?\n","\n","\n","# Create data loaders\n","train_loader = torch.utils.data.DataLoader(\n","    dataset     = train_dataset,\n","    batch_size  = config['batch_size'],\n","    shuffle     = True,\n","    num_workers = 2,\n","    pin_memory  = True\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    dataset     = valid_dataset,\n","    batch_size  = config['batch_size'],\n","    shuffle     = False,\n","    num_workers = 2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqSR063BGE2e","trusted":true},"outputs":[],"source":["# You can do this with ImageFolder as well, but it requires some tweaking\n","class ClassificationTestDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, transforms):\n","        self.data_dir   = data_dir\n","        self.transforms = transforms\n","\n","        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n","        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        return self.transforms(Image.open(self.img_paths[idx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVLB41KtGC2o","trusted":true},"outputs":[],"source":["test_dataset = ClassificationTestDataset(TEST_DIR, transforms = valid_transforms) #Why are we using val_transforms for Test Data?\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n","                         drop_last = False, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4t8eU9gY0Jy","outputId":"062bbbcc-31fd-4974-ba39-f069a472cf3e","trusted":true},"outputs":[],"source":["print(\"Number of classes    : \", len(train_dataset.classes))\n","print(\"No. of train images  : \", train_dataset.__len__())\n","print(\"Shape of image       : \", train_dataset[0][0].shape)\n","print(\"Batch size           : \", config['batch_size'])\n","print(\"Train batches        : \", train_loader.__len__())\n","print(\"Val batches          : \", valid_loader.__len__())"]},{"cell_type":"markdown","metadata":{"id":"zs2Xw_tl0IQ8"},"source":["## Data visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIoRUzCbz85y","outputId":"f84ce8fd-7f5a-49a8-8bb9-2187c861d62b","trusted":true},"outputs":[],"source":["# Visualize a few images in the dataset\n","# You can write your own code, and you don't need to understand the code\n","# It is highly recommended that you visualize your data augmentation as sanity check\n","\n","r, c    = [5, 5]\n","fig, ax = plt.subplots(r, c, figsize= (15, 15))\n","\n","k       = 0\n","dtl     = torch.utils.data.DataLoader(\n","    dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms\n","    batch_size  = config['batch_size'],\n","    shuffle     = True,\n",")\n","\n","for data in dtl:\n","    x, y = data\n","\n","    for i in range(r):\n","        for j in range(c):\n","            img = x[k].numpy().transpose(1, 2, 0)\n","            ax[i, j].imshow(img)\n","            ax[i, j].axis('off')\n","            k+=1\n","    break\n","\n","del dtl"]},{"cell_type":"markdown","metadata":{"id":"mIqmojPaWD0H"},"source":["# High cut-off Network"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SqueezeExcitationBlock(nn.Module):\n","    def __init__(self, in_channels, reduction_ratio=16):\n","        super(SqueezeExcitationBlock, self).__init__()\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        out = self.global_avg_pool(x).squeeze(-1).squeeze(-1)\n","        out = self.fc1(out)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        out = self.sigmoid(out)\n","        out = out.unsqueeze(-1).unsqueeze(-1)\n","        return x * out\n","\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.se_block = SqueezeExcitationBlock(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        out = self.se_block(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","class SEResNet(nn.Module):\n","    def __init__(self, block, layers, num_classes=1000):\n","        super(SEResNet, self).__init__()\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def _make_layer(self, block, out_channels, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_channels != out_channels:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        layers = [block(self.in_channels, out_channels, stride, downsample)]\n","        self.in_channels = out_channels\n","\n","        for _ in range(1, blocks):\n","            layers.append(block(out_channels, out_channels))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, return_feats=False):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.global_avg_pool(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.fc(x)\n","\n","        if return_feats:\n","            return out, x  \n","        else:\n","            return out\n","\n","model = SEResNet(BasicBlock, [4, 5, 6, 2], num_classes=7001).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbnVmZ989cKv","trusted":true},"outputs":[],"source":["def count_parameters(model=model):\n","    params = [p.numel() for p in model.parameters() if p.requires_grad]\n","    print(f'Model total parameters: {sum(params):>7}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_parameters()"]},{"cell_type":"markdown","metadata":{"id":"KZCn0qHuZRKj"},"source":["# Setup everything for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UowI9OcUYPjP","trusted":true},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.2) # TODO: What loss do you need for a multi class classification problem?\n","optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=1e-4)\n","# TODO: Implement a scheduler (Optional but Highly Recommended)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=0.00004)\n","# You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n","scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n","# It is useful only in the case of compatible GPUs such as T4/V100"]},{"cell_type":"markdown","metadata":{"id":"dzM11HtcboYv"},"source":["# Let's train!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgSw6iJJavBZ","trusted":true},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion):\n","\n","    model.train()\n","\n","    # Progress Bar\n","    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n","\n","    num_correct = 0\n","    total_loss  = 0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        optimizer.zero_grad() # Zero gradients\n","\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n","            outputs = model(images)\n","            loss    = criterion(outputs, labels)\n","\n","        # Update no. of correct predictions & loss as we iterate\n","        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss      += float(loss.item())\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct = num_correct,\n","            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n","        )\n","\n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update()\n","\n","        # TODO? Depending on your choice of scheduler,\n","\n","        # You may want to call some schdulers inside the train function. What are these?\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss  = float(total_loss / len(dataloader))\n","\n","    return acc, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5V2UdnpdEoK","trusted":true},"outputs":[],"source":["def validate(model, dataloader, criterion):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n","\n","    num_correct = 0.0\n","    total_loss = 0.0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        # Move images to device\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss += float(loss.item())\n","\n","        batch_bar.set_postfix(\n","            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct=num_correct)\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss = float(total_loss / len(dataloader))\n","    return acc, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmotca6pcLLY","trusted":true},"outputs":[],"source":["gc.collect() # These commands help you when you face CUDA OOM error\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"2mBgKGkXLrdJ"},"source":["# Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix62_BkaLr_D","outputId":"ef167073-eb66-4056-f04d-c400b3c43770","trusted":true},"outputs":[],"source":["wandb.login(key=\"<key>\") #API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = \"high-cut-off\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # id = '3x9jp5uz', # Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VG0vmsmbRYEi","outputId":"c85d892e-b10a-4093-9a4a-0598ee731ef2","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SQkRw1FvLqYe"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!mkdir \"/kaggle/working/models\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqWO8Edb0BK2","outputId":"db8278b1-a7f4-4685-f137-141697d46a1a","trusted":true},"outputs":[],"source":["best_valacc = 0.0\n","root = '/kaggle/working/'\n","\n","model_directory = os.path.join(root, \"models\")\n","\n","for epoch in range(config['epochs']):\n","\n","    curr_lr = float(optimizer.param_groups[0]['lr'])\n","\n","    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n","\n","    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n","        epoch + 1,\n","        config['epochs'],\n","        train_acc,\n","        train_loss,\n","        curr_lr))\n","\n","    val_acc, val_loss = validate(model, valid_loader, criterion)\n","\n","    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n","\n","    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc,\n","               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n","\n","    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n","    # your learning rate differently\n","    scheduler.step()\n","\n","    # #Save model in drive location if val_acc is better than best recorded val_acc\n","    if val_acc >= best_valacc:\n","        path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n","        print(\"Saving model\")\n","        torch.save({'model_state_dict':model.state_dict(),\n","                  'optimizer_state_dict':optimizer.state_dict(),\n","                  'scheduler_state_dict':scheduler.state_dict(),\n","                  'val_acc': val_acc,\n","                  'epoch': epoch}, path)\n","        best_valacc = val_acc\n","        wandb.save(path)\n","      # You may find it interesting to exlplore Wandb Artifcats to version your models\n","run.finish()"]},{"cell_type":"markdown","metadata":{"id":"UpgCHImRkYQW"},"source":["# Classification Task: Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSSJtjs3BqBx","trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load(path)['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2WQEUjXkWvo","trusted":true},"outputs":[],"source":["def test(model, dataloader):\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n","    test_results = []\n","\n","    for i, (images) in enumerate(dataloader):\n","        # TODO: Finish predicting on the test set.\n","        images = images.to(DEVICE)\n","\n","        # Set the model to evaluation mode and use torch.no_grad()\n","        with torch.no_grad():\n","            outputs = model(images)\n","\n","        outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n","        test_results.extend(outputs)\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    return test_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7R1lcCAzULc","outputId":"603e5e43-8a6c-4ae9-b9b5-0349a1b44e18","trusted":true},"outputs":[],"source":["test_results = test(model, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"zqfUzwS2L1gx"},"source":["## Generate csv to submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vob9a2-HkW_V","trusted":true},"outputs":[],"source":["with open(\"classification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(test_dataset)):\n","        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnRUN53CZMTf","outputId":"93e9c55e-7507-495d-b34c-8cf1f69c9550","trusted":true},"outputs":[],"source":["!kaggle competitions submit -c 11-785-f23-hw2p2-classification -f classification_early_submission.csv -m \"early submission\""]},{"cell_type":"markdown","metadata":{"id":"7WYgUjJzUiGU"},"source":["# Verification Task: Validation"]},{"cell_type":"markdown","metadata":{"id":"FoBFFF8-Lpvj"},"source":["The verification task consists of the following generalized scenario:\n","- You are given X unknown identitites\n","- You are given Y known identitites\n","- Your goal is to match X unknown identities to Y known identities.\n","\n","We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.\n","\n","Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities.\n","\n","This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9aY5o-suWdn","outputId":"9e8906e8-9ddc-4cae-f1a6-e54a17b4f371","trusted":true},"outputs":[],"source":["# This obtains the list of known identities from the known folder\n","known_regex = \"/kaggle/input/11-785-f23-hw2p2-verification/11-785-f23-hw2p2-verification/known/*/*\"\n","known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n","\n","# Obtain a list of images from unknown folders\n","unknown_dev_regex = \"/kaggle/input/11-785-f23-hw2p2-verification/11-785-f23-hw2p2-verification/unknown_dev/*\"\n","unknown_test_regex = \"/kaggle/input/11-785-f23-hw2p2-verification/11-785-f23-hw2p2-verification/unknown_test/*\"\n","\n","# We load the images from known and unknown folders\n","unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n","unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n","known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n","\n","# Why do you need only ToTensor() here?\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor()])\n","\n","unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n","unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n","known_images  = torch.stack([transforms(y) for y in known_images ])\n","#Print your shapes here to understand what we have done\n","print('unknown_dev_images\\t', unknown_dev_images.shape)\n","print('unknown_test_images\\t', unknown_test_images.shape)\n","print('known_images\\t', known_images.shape)\n","# You can use other similarity metrics like Euclidean Distance if you wish\n","similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'):\n","\n","    unknown_feats, known_feats = [], []\n","\n","    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n","    model.eval()\n","\n","    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n","    for i in range(0, unknown_images.shape[0], batch_size):\n","        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n","\n","        with torch.no_grad():\n","            _, unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model\n","        unknown_feats.append(unknown_feat)\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","\n","    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n","\n","    for i in range(0, known_images.shape[0], batch_size):\n","        known_batch = known_images[i:i+batch_size]\n","        with torch.no_grad():\n","              _, known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n","\n","        known_feats.append(known_feat)\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","\n","    # Concatenate all the batches\n","    unknown_feats = torch.cat(unknown_feats, dim=0)\n","    known_feats = torch.cat(known_feats, dim=0)\n","\n","    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n","    # Print the inner list comprehension in a separate cell - what is really happening?\n","\n","    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?\n","    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n","\n","\n","    # Note that in unknown identities, there are identities without correspondence in known identities.\n","    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain\n","    # threshold compared with those identities with correspondence.\n","\n","    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value\n","    # pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings\n","\n","    # After early submission, remove the previous line and uncomment the following code\n","\n","    threshold = 0.5 # Choose a proper threshold\n","    NO_CORRESPONDENCE_LABEL = 'n000000'\n","    pred_id_strings = []\n","    for idx, prediction in enumerate(predictions):\n","        if max_similarity_values[idx] < threshold: # why < ? Think about what is your similarity metric\n","            pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n","        else:\n","            pred_id_strings.append(known_paths[prediction])\n","\n","    if mode == 'val':\n","      true_ids = pd.read_csv('/kaggle/input/11-785-f23-hw2p2-verification/11-785-f23-hw2p2-verification/verification_dev.csv')['label'].tolist()\n","      accuracy = accuracy_score(pred_id_strings, true_ids)\n","      print(\"Verification Accuracy = {}\".format(accuracy))\n","\n","    return pred_id_strings"]},{"cell_type":"markdown","metadata":{},"source":["### Retraining Model for Verification using Center Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CenterLoss(nn.Module):\n","    \"\"\"Center Loss\n","    Center Loss Paper:\n","    https://ydwen.github.io/papers/WenECCV16.pdf\n","    Args:\n","        num_classes (int): The number of classes for your model.\n","        feat_dim (int): The dimension of your output feature.\n","    \"\"\"\n","    def __init__(self, num_classes=7001, feat_dim=512):\n","        super(CenterLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.feat_dim = feat_dim\n","\n","        # Initialize centers for each class.\n","        # The centers are learnable parameters, and you need to use the nn.Parameter\n","        # so that they are registered as model parameters.\n","        # We initialize them using random values, and they are moved to GPU.\n","        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n","\n","    def forward(self, x, labels):\n","        \"\"\"\n","        Args:\n","            x: Feature matrix with shape (batch_size, feat_dim).\n","            labels: Ground truth labels with shape (batch_size).\n","        \"\"\"\n","        # Broadcast the centers for each input based on the labels.\n","        # This will create a tensor where centers[i] will contain the center of the true label of x[i].\n","        centers_batch = self.centers[labels]\n","\n","        # Calculate the squared Euclidean distances between inputs and current centers.\n","        dist = torch.sum((x - centers_batch) ** 2, dim=1)\n","\n","        # Clamp the distances to avoid NaN in log and to provide numerical stability.\n","        dist = torch.clamp(dist, min=1e-12, max=1e+12)\n","\n","        # Calculate the mean loss across the batch.\n","        loss = torch.mean(dist)\n","\n","        return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.2) # TODO: What loss do you need for a multi class classification problem?\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize the CenterLoss and its optimizer\n","center_loss = CenterLoss(num_classes=7001, feat_dim=512)\n","optimizer_center_loss = torch.optim.SGD(center_loss.parameters(), lr=0.1)\n","scheduler_center_loss = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_center_loss, T_max=150, eta_min=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_l(model: nn.Module, \n","          train_loader: torch.utils.data.DataLoader, \n","          optimizer: torch.optim.Optimizer, \n","          optimizer_center_loss: torch.optim.Optimizer, \n","          criterion: nn.Module, \n","          fine_tuning_loss: nn.Module,  # Center Loss as fine_tuning_loss\n","          loss_weight, \n","          \n","          scaler: torch.cuda.amp.GradScaler, \n","          device):\n","    \n","    model.train()\n","    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=6)\n","\n","    num_correct = 0\n","    total_loss_ft  = 0\n","    total_loss  = 0\n","    \n","    for i, (images, labels) in enumerate(train_loader):\n","        \n","        optimizer.zero_grad()\n","        optimizer_center_loss.zero_grad()\n","        \n","        images, labels = images.to(device), labels.to(device)\n","\n","        with torch.cuda.amp.autocast():\n","            outputs, feats = model(images, return_feats=True)\n","            loss0 = criterion(outputs, labels)  # Calculate cross-entropy loss\n","            loss1 = fine_tuning_loss(feats, labels) * loss_weight  # Calculate weighted fine-tuning loss (Center Loss)\n","            \n","            \n","        # Update no. of correct predictions & loss as we iterate\n","        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss      += float(loss0.item())\n","        total_loss_ft      += float(loss1.item())\n","        \n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n","            loss_ft        = \"{:.04f}\".format(float(total_loss_ft / (i + 1))),\n","            num_correct = num_correct,\n","            lr          = \"{:.04f}\".format(float(optimizer_center_loss.param_groups[0]['lr']))\n","        )\n","        \n","        \n","\n","        scaler.scale(loss0).backward(retain_graph=True)  # Backward pass for the classification loss\n","        scaler.scale(loss1).backward()  # Backward pass for the fine-tuning loss\n","        \n","        # update fine tuning loss' parameters\n","        # the paramerters should be adjusted according to the loss_weight you choose\n","        for parameter in fine_tuning_loss.parameters():\n","            parameter.grad.data *= (1.0 / loss_weight)\n","\n","        scaler.step(optimizer_center_loss)  # Step optimizer for fine-tuning loss\n","        scaler.step(optimizer)  # Step optimizer for classification loss\n","        scaler.update()\n","    \n","        \n","        batch_bar.update() # Update tqdm bar\n","        \n","        del images, labels, outputs, loss0, loss1\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    acc         = 100 * num_correct / (config['batch_size']* len(train_loader))\n","    total_loss  = float(total_loss / len(train_loader))\n","    total_loss_ft  = float(total_loss_ft / len(train_loader))\n","    \n","    return acc, total_loss, total_loss_ft"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def validate_l(model, dataloader, criterion, fine_tuning_loss, loss_weight):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=6)\n","\n","    num_correct = 0.0\n","    total_loss = 0.0\n","    total_loss_ft = 0.0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        # Move images to device\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs, feats = model(images, return_feats=True)\n","            loss = criterion(outputs, labels)\n","            loss_ft = fine_tuning_loss(feats, labels)* loss_weight\n","\n","        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss += float(loss.item())\n","        total_loss_ft      += float(loss_ft.item())\n","\n","        batch_bar.set_postfix(\n","            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            loss_ft=\"{:.04f}\".format(float(total_loss_ft / (i + 1))),\n","            num_correct=num_correct)\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss = float(total_loss / len(dataloader))\n","    total_loss_ft  = float(total_loss_ft / len(dataloader))\n","    return acc, total_loss, total_loss_ft"]},{"cell_type":"markdown","metadata":{},"source":["### Wandb run for fine tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = \"high-cut-off-finetune\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # id = 'c8vpf2v8', # Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["config['epochs']=97"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loss_weight = 0.0025\n","\n","best_val_acc = 90.7\n","root = '/kaggle/working/'\n","\n","model_directory = os.path.join(root, \"models\")\n","\n","for epoch in range(config['epochs']):\n","\n","    curr_lr = float(optimizer_center_loss.param_groups[0]['lr'])\n","\n","    train_acc, train_loss, ft_loss = train_l(model, train_loader, optimizer, optimizer_center_loss, criterion, center_loss, loss_weight, scaler, DEVICE)\n","\n","\n","    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t FT train loss {:.04f}\\t Learning Rate ft {:.04f}\".format(\n","        epoch + 1,\n","        config['epochs'],\n","        train_acc,\n","        train_loss,\n","        ft_loss,\n","        curr_lr))\n","\n","    val_acc, val_loss, val_loss_ft = validate_l(model, valid_loader, criterion, center_loss, loss_weight)\n","\n","    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\\t FT Val Loss {:.04f}\".format(val_acc, val_loss, val_loss_ft))\n","\n","    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, 'ft_loss' :val_loss_ft,\n","               'validation_loss': val_loss, \"learning_Rate_ft\": curr_lr})\n","\n","    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n","    # your learning rate differently\n","    \n","    scheduler_center_loss.step()\n","\n","    # #Save model in drive location if val_acc is better than best recorded val_acc\n","    if val_acc >= best_val_acc:\n","        path = os.path.join(root, model_directory, 'checkpoint.pth')\n","        print(\"Saving model\")\n","        torch.save({'model_state_dict':model.state_dict(),\n","                  'optimizer_state_dict':optimizer.state_dict(),\n","                  'optimizer_center_loss_state_dict':optimizer_center_loss.state_dict(),\n","                  'scheduler_center_loss_state_dict':scheduler_center_loss.state_dict(),\n","                  'val_acc': val_acc,\n","                  'epoch': epoch}, path)\n","        best_val_acc = val_acc\n","        wandb.save(path)\n","      # You may find it interesting to exlplore Wandb Artifcats to version your models\n","run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load(path)['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMC7FacaUnJ7","outputId":"a876886a-b94e-4ae5-94db-d981129680e7","trusted":true},"outputs":[],"source":["# verification eval\n","pred_id_strings = eval_verification(unknown_dev_images, known_images, model, similarity_metric, config['batch_size'], mode='val')\n","# verification test\n","pred_id_strings = eval_verification(unknown_test_images, known_images, model, similarity_metric, config['batch_size'], mode='test')"]},{"cell_type":"markdown","metadata":{"id":"iTLW0RPD7XGC"},"source":["## Generate csv to submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fD-r-HmsAeWV","trusted":true},"outputs":[],"source":["with open(\"verification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(pred_id_strings)):\n","        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPIgq0tMZ8qk","outputId":"085870d2-874a-4ac5-de58-3a2d485255b4","trusted":true},"outputs":[],"source":["!kaggle competitions submit -c 11-785-f23-hw2p2-verification -f verification_early_submission.csv -m \"early submission\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
