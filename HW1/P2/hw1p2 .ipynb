{"cells":[{"cell_type":"markdown","metadata":{"id":"F9ERgBpbcMmB"},"source":["# HW1: Frame-Level Speech Recognition"]},{"cell_type":"markdown","metadata":{"id":"CLkH6GMGcWcE"},"source":["In this homework, you will be working with MFCC data consisting of 28 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."]},{"cell_type":"markdown","metadata":{},"source":["To run this notebook, follow the cells chronological order. Also, note that this notebook was ran on Kaggle notebook, you will have to add competition dataset. The notebook is mainly made up of the following sections:\n","- Install and Import libraries\n","- Install Kaggle API\n","- Datasets and Dataloaders\n","- Parameter configuration\n","- Network architecture\n","- Criterion and optimizer\n","- Training and validation functions\n","- Training and logging checkpoint and hyperparameters on WandB\n","- Testing and Kaggle submission\n","#### Dataset and dataloaders\n","To improve memory efficiency, arrays were created with shapes equal to the final concatenated MFCCs + context and transcripts. These arrays were pre-filled with zeros. Instead of appending each frame to a list and then concatenating, which consumes almost double the memory, I directly loaded each frame and inserted it into the previously created array using indexing and slicing.\n","#### Architectures\n","Here are few of many architectures that I have tried. I have used a fixed learning rate of 1e-3.\n","- Diamond architecture: \n","- - Activation: ReLU \n","- - Optimizer: ADAM\n","- - Context: 20\n","- - 3 hidden layers with width 512, 1024, 512\n","- - Results: 76%\n","- Diamond architecture: \n","- - Activation: ReLU \n","- - Optimizer: ADAM\n","- - Context: 20\n","- - 5 hidden layers with width 512, 1024, 2048, 1024, 512\n","- - Results: 79.5%\n","- Diamond architecture: \n","- - Activation: ReLU \n","- - Optimizer: ADAM\n","- - Batch normalization after every layer\n","- - Dropout of 0.5 after every layer\n","- - Context: 30\n","- - 5 hidden layers with width 512, 1024, 2048, 1024, 512\n","- - Results: 81% acuracy on validation but terrible on Kaggle test set\n","- Multi-stage Diamond architecture: \n","- - Activation: ReLU  \n","- - Optimizer: ADAM \n","- - Batchnorm after every alternate layer\n","- - Context: 30\n","- - 7 hidden layers of width 1024, 2048, 2048, 1024, 2024, 512, 512\n","- - Results: Overfit\n","- Cylinder architecture of width 2048: \n","- - Activation: SiLu \n","- - Optimizer: ADAMW\n","- - weight_decay: 1e-4\n","- - 5 hidden layers of width 2048\n","- - dropout of 0.2 after every layer\n","- - Context: 30\n","- - Batchnorm after every alternate layer\n","- -  Results: stack at 84.5% accuracy.\n","- Cylinder architecture: \n","- - Activation: SiLu \n","- - Optimizer: ADAMW\n","- - weight_decay: 1e-4\n","- - 4 hidden layers of width 3072\n","- - dropout of 0.2 after every layer\n","- - Batchnorm after every alternate layer\n","- -  Results: stack around 82% accuracy.\n","- Cylinder architecture of width 2048: \n","- - Activation: SiLu \n","- - Optimizer: ADAMW\n","- - weight_decay: 1e-4\n","- - batch size: 8000\n","- - Context: 30\n","- - 6 hidden layers of width 2048\n","- - Dropout of 0.2 after every layer\n","- - Batchnorm after every alternate layer\n","- -  Results: reached around 83% accuracy then start overfitting\n","##### The following model is the one that gave me the best results\n","\n","- Cylinder architecture: \n","- - Activation: ReLu \n","- - Optimizer: ADAMW\n","- - Batch size: 8192\n","- - Epochs: 80\n","- - Context: 30\n","- - weight_decay: 1e-4\n","- - 6 hidden layers of width 2048\n","- - Dropout of 0.3 for the first 2 layers then 0.2 for the rest\n","- - Batchnorm after every alternate layer\n","- - 24,580,138 parameters\n","- -  Results: reached around 86.2% validation accuracy.\\\n","\n","When training these models, I saved the checkpoints lowest lost model, best accuracy model and current model. The best accuracy model was used for test prediction.\n"]},{"cell_type":"markdown","metadata":{"id":"z4vZbDmJvMp1"},"source":["# Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:04.812125Z","iopub.status.busy":"2023-09-24T20:40:04.811864Z","iopub.status.idle":"2023-09-24T20:40:19.857912Z","shell.execute_reply":"2023-09-24T20:40:19.856629Z","shell.execute_reply.started":"2023-09-24T20:40:04.812100Z"},"id":"rwYu9sSUnSho","outputId":"ae19c201-5064-44bc-f5d5-86c8dffb94ad","trusted":true},"outputs":[],"source":["!pip install torchsummaryX wandb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:19.862677Z","iopub.status.busy":"2023-09-24T20:40:19.862383Z","iopub.status.idle":"2023-09-24T20:40:24.856382Z","shell.execute_reply":"2023-09-24T20:40:24.855103Z","shell.execute_reply.started":"2023-09-24T20:40:19.862650Z"},"id":"qI4qfx7tiBZt","outputId":"fd0e810a-cfec-4b56-af7a-2f635ade05f7","trusted":true},"outputs":[],"source":["import torch\n","from torch.cuda.amp import autocast, GradScaler\n","import numpy as np\n","from torchsummaryX import summary\n","import sklearn\n","import gc\n","import zipfile\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import os\n","import datetime\n","import wandb\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:24.858624Z","iopub.status.busy":"2023-09-24T20:40:24.857962Z","iopub.status.idle":"2023-09-24T20:40:24.863173Z","shell.execute_reply":"2023-09-24T20:40:24.862211Z","shell.execute_reply.started":"2023-09-24T20:40:24.858587Z"},"id":"8yBgXjKV1O0Z","outputId":"c478a952-7fb5-4294-acc9-57080d85a213","trusted":true},"outputs":[],"source":["# ### If you are using colab, you can import google drive to save model checkpoints in a folder\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:24.866701Z","iopub.status.busy":"2023-09-24T20:40:24.865989Z","iopub.status.idle":"2023-09-24T20:40:24.875846Z","shell.execute_reply":"2023-09-24T20:40:24.875247Z","shell.execute_reply.started":"2023-09-24T20:40:24.866668Z"},"id":"N-9qE20hmCgQ","trusted":true},"outputs":[],"source":["### PHONEME LIST\n","PHONEMES = [\n","            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n","            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n","            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n","            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n","            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n","            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"]},{"cell_type":"markdown","metadata":{"id":"ZIi0Big7vPa9"},"source":["# Kaggle"]},{"cell_type":"markdown","metadata":{"id":"BBCbeRhixGM7"},"source":["This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:24.877915Z","iopub.status.busy":"2023-09-24T20:40:24.877219Z","iopub.status.idle":"2023-09-24T20:40:31.844348Z","shell.execute_reply":"2023-09-24T20:40:31.842972Z","shell.execute_reply.started":"2023-09-24T20:40:24.877884Z"},"id":"TPBUd7Cnl-Rx","outputId":"fb3d1c49-9eaa-4600-a967-c256ac612c3e","trusted":true},"outputs":[],"source":["!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"gbwiraye\",\"key\":\"<Kaggle API key>\"}')\n","    # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:31.846887Z","iopub.status.busy":"2023-09-24T20:40:31.846463Z","iopub.status.idle":"2023-09-24T20:40:31.852742Z","shell.execute_reply":"2023-09-24T20:40:31.851147Z","shell.execute_reply.started":"2023-09-24T20:40:31.846846Z"},"id":"if2Somqfbje1","outputId":"a4e4ea45-cf99-4d9e-9adc-8545b6244729","trusted":true},"outputs":[],"source":["# # commands to download data from kaggle\n","\n","# !kaggle competitions download -c 11785-hw1p2-f23\n","# !mkdir '/kaggle/working/data'\n","\n","# !unzip -qo /kaggle/working/11785-hw1p2-f23.zip -d '/kaggle/working/data'"]},{"cell_type":"markdown","metadata":{"id":"Vuzce0_TdcaR"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"2_7QgMbBdgPp"},"source":["This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n","\n","Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:31.854786Z","iopub.status.busy":"2023-09-24T20:40:31.854434Z","iopub.status.idle":"2023-09-24T20:40:31.872647Z","shell.execute_reply":"2023-09-24T20:40:31.871397Z","shell.execute_reply.started":"2023-09-24T20:40:31.854739Z"},"id":"j99sEKSJiARE","trusted":true},"outputs":[],"source":["class AudioDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, root, phonemes = PHONEMES, context =0, partition = \"train-clean-100\"):\n","\n","        self.context = context\n","        self.phonemes = phonemes\n","\n","        # MFCC directory - use partition to acces train/dev directories from kaggle data using root\n","        self.mfcc_dir       = os.path.join(root, partition,'mfcc')\n","        # Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n","        self.transcript_dir = os.path.join(root, partition,'transcript')\n","\n","        #  List files in sefl.mfcc_dir using os.listdir in sorted order\n","        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n","        # List files in self.transcript_dir using os.listdir in sorted order\n","        transcript_names    = sorted(os.listdir(self.transcript_dir))\n","\n","        # Making sure that we have the same no. of mfcc and transcripts\n","        assert len(mfcc_names) == len(transcript_names)\n","\n","        length = len(mfcc_names)\n","        T = 0\n","\n","        for i in range(length):\n","            #   Load a single mfcc\n","            mfcc  = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n","            # Extract the length of the mfcc\n","            T += mfcc.shape[0]\n","\n","        self.mfccs = np.zeros((2 * self.context + T, 28),dtype = np.float32)\n","        self.transcripts = np.zeros((T,), dtype = np.uint8)\n","        # Encoding phonemes\n","        PHONEMES_endoded = {p:i for i, p in enumerate(self.phonemes)}\n","\n","        cy, cx = 0,self.context\n","\n","        for i in range(length):\n","            #   Load a single mfcc\n","            mfcc        = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n","            #   Do Cepstral Normalization of mfcc (explained in writeup)\n","            mfcc = (mfcc-np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0) #Added axis =0, as advised by the TA on Piazza\n","            #Watch out for an issue with the datatype, needs to be tensors not numpy arrays\n","            #   Load the corresponding transcript and Remove [SOS] and [EOS] from the transcript\n","            transcript  = np.load(os.path.join(self.transcript_dir, transcript_names[i]))[1:-1]\n","            #  Convert transcript to a sequence of integers based on self.phonemes\n","            y = [PHONEMES_endoded[phoneme] for phoneme in transcript]\n","\n","            self.mfccs[cx:cx + mfcc.shape[0]] = mfcc\n","            self.transcripts[cy:cy + mfcc.shape[0]] = y\n","\n","            cx += mfcc.shape[0]\n","            cy += mfcc.shape[0]\n","\n","        self.length = T\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","\n","        # Based on context and offset, return a frame at given index with context frames to the left, and right.\n","        frames = self.mfccs[ind : ind + 2 * self.context + 1]\n","        # After slicing, you get an array of shape 2*context+1 x 28. But our MLP needs 1d data and not 2d.\n","        frames = frames.flatten() # Flatten to get 1d data\n","\n","        frames      = torch.FloatTensor(frames) # Convert to tensors\n","        phonemes    = torch.tensor(self.transcripts[ind])\n","\n","        return frames, phonemes"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:31.874743Z","iopub.status.busy":"2023-09-24T20:40:31.874317Z","iopub.status.idle":"2023-09-24T20:40:31.889308Z","shell.execute_reply":"2023-09-24T20:40:31.888314Z","shell.execute_reply.started":"2023-09-24T20:40:31.874645Z"},"id":"NAqQfCuAvlyV","trusted":true},"outputs":[],"source":["class AudioTestDataset(torch.utils.data.Dataset):\n","\n","    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n","    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n","    def __init__(self, root, context=0, partition= \"test-clean\"):\n","\n","        self.context = context\n","\n","        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n","        self.mfcc_dir       = os.path.join(root, partition,'mfcc')\n","\n","        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n","        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n","\n","        length = len(mfcc_names)\n","        T = 0\n","\n","        for i in range(length):\n","            #   Load a single mfcc\n","            mfcc  = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n","            # Extract the length of the mfcc\n","            T += mfcc.shape[0]\n","\n","        self.mfccs = np.zeros((2 * self.context + T, 28),dtype = np.float32)\n","\n","        cx = self.context\n","\n","        for i in range(length):\n","            #   Load a single mfcc\n","            mfcc        = np.load(os.path.join(self.mfcc_dir, mfcc_names[i]))\n","            #   Do Cepstral Normalization of mfcc (explained in writeup)\n","            mfcc = (mfcc-np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0) #Added axis =0, as advised by the TA on Piazza\n","            #Watch out for an issue with the datatype, needs to be tensors not numpy arrays\n","\n","            self.mfccs[cx:cx + mfcc.shape[0]] = mfcc\n","            cx += mfcc.shape[0]\n","\n","        self.length = T\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","\n","        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n","        frames = self.mfccs[ind : ind + 2 * self.context + 1]\n","        # After slicing, you get an array of shape 2*context+1 x 28. But our MLP needs 1d data and not 2d.\n","        frames = frames.flatten() # TODO: Flatten to get 1d data\n","\n","        frames      = torch.FloatTensor(frames) # Convert to tensors\n","\n","        return frames"]},{"cell_type":"markdown","metadata":{"id":"qNacQ8bpt9nw"},"source":["\n","\n","# Parameters Configuration"]},{"cell_type":"markdown","metadata":{"id":"WE7tsinAuLNy"},"source":["Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:31.910411Z","iopub.status.busy":"2023-09-24T20:40:31.910119Z","iopub.status.idle":"2023-09-24T20:40:31.917443Z","shell.execute_reply":"2023-09-24T20:40:31.916542Z","shell.execute_reply.started":"2023-09-24T20:40:31.910388Z"},"trusted":true},"outputs":[],"source":["config = {\n","    'epochs': 80,\n","    'batch_size': 8192,\n","    'context': 30,\n","    'init_lr': 1e-3,\n","    'architecture': 'high-cutoff',\n","    'weight_decay': 1e-4,  # L2 regularization strength\n","}\n"]},{"cell_type":"markdown","metadata":{"id":"2mlwaKlDt_2c"},"source":["# Create Datasets"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:40:31.919695Z","iopub.status.busy":"2023-09-24T20:40:31.919012Z","iopub.status.idle":"2023-09-24T20:48:33.895041Z","shell.execute_reply":"2023-09-24T20:48:33.893990Z","shell.execute_reply.started":"2023-09-24T20:40:31.919664Z"},"id":"7xi7V8x8W9z4","trusted":true},"outputs":[],"source":["#TODO: Create a dataset object using the AudioDataset class for the training data \n","train_data = AudioDataset(root='/kaggle/input/11785-hw1p2-f23/11-785-f23-hw1p2/', context=config['context'])\n","\n","# TODO: Create a dataset object using the AudioDataset class for the validation data\n","val_data = AudioDataset(root='/kaggle/input/11785-hw1p2-f23/11-785-f23-hw1p2/', partition='dev-clean', context=config['context'])\n","# TODO: Create a dataset object using the AudioTestDataset class for the test data\n","test_data = AudioTestDataset(root='/kaggle/input/11785-hw1p2-f23/11-785-f23-hw1p2/', partition='test-clean', context=config['context'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:33.897295Z","iopub.status.busy":"2023-09-24T20:48:33.896592Z","iopub.status.idle":"2023-09-24T20:48:33.909241Z","shell.execute_reply":"2023-09-24T20:48:33.908213Z","shell.execute_reply.started":"2023-09-24T20:48:33.897262Z"},"id":"4mzoYfTKu14s","outputId":"674d3ae2-ca64-4d34-83b4-eae9ef37cd7a","trusted":true},"outputs":[],"source":["# Define dataloaders for train, val and test datasets\n","# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n","# We shuffle train dataloader but not val & test dataloader. Why?\n","\n","train_loader = torch.utils.data.DataLoader(\n","    dataset     = train_data,\n","    num_workers = 2,\n","    batch_size  = config['batch_size'],\n","    pin_memory  = True,\n","    shuffle     = True\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    dataset     = val_data,\n","    num_workers = 2,\n","    batch_size  = config['batch_size'],\n","    pin_memory  = True,\n","    shuffle     = False\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    dataset     = test_data,\n","    num_workers = 2,\n","    batch_size  = config['batch_size'],\n","    pin_memory  = True,\n","    shuffle     = False\n",")\n","\n","\n","print(\"Batch size     : \", config['batch_size'])\n","print(\"Context        : \", config['context'])\n","print(\"Input size     : \", (2*config['context']+1)*28)\n","print(\"Output symbols : \", len(PHONEMES))\n","\n","print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n","print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n","print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:33.911118Z","iopub.status.busy":"2023-09-24T20:48:33.910758Z","iopub.status.idle":"2023-09-24T20:48:44.700535Z","shell.execute_reply":"2023-09-24T20:48:44.699442Z","shell.execute_reply.started":"2023-09-24T20:48:33.911087Z"},"id":"n-GV3UvgLSoF","outputId":"f0ec89d5-10fc-4ee9-c847-ab88984119d5","trusted":true},"outputs":[],"source":["# Testing code to check if your data loaders are working\n","for i, data in enumerate(train_loader):\n","    frames, phoneme = data\n","    print(frames.shape, phoneme.shape)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"Nxjwve20JRJ2"},"source":["# Network Architecture\n"]},{"cell_type":"markdown","metadata":{"id":"3NJzT-mRw6iy"},"source":["This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:44.712204Z","iopub.status.busy":"2023-09-24T20:48:44.711851Z","iopub.status.idle":"2023-09-24T20:48:44.736639Z","shell.execute_reply":"2023-09-24T20:48:44.735738Z","shell.execute_reply.started":"2023-09-24T20:48:44.712173Z"},"trusted":true},"outputs":[],"source":["class Network(torch.nn.Module):\n","\n","    def __init__(self, input_size, output_size):\n","\n","        super(Network, self).__init__()\n","\n","        self.model = torch.nn.Sequential(\n","            torch.nn.Linear(input_size, 2048),\n","            torch.nn.BatchNorm1d(2048),\n","            torch.nn.GELU(),\n","            torch.nn.Dropout(0.3),\n","\n","            torch.nn.Linear(2048, 2048),\n","            torch.nn.GELU(),\n","            torch.nn.Dropout(0.3),\n","\n","            torch.nn.Linear(2048, 2048),\n","            torch.nn.BatchNorm1d(2048),\n","            torch.nn.GELU(),\n","            torch.nn.Dropout(0.2),\n","\n","            torch.nn.Linear(2048, 2048),\n","            torch.nn.GELU(),\n","            torch.nn.Dropout(0.2),\n","\n","            torch.nn.Linear(2048, 2048),\n","            torch.nn.BatchNorm1d(2048),\n","            torch.nn.GELU(),\n","            torch.nn.Dropout(0.2),\n","\n","            torch.nn.Linear(2048, 2048),\n","            torch.nn.GELU(),\n","            torch.nn.Dropout(0.2),\n","\n","            torch.nn.Linear(2048, output_size)\n","        )\n","\n","    def forward(self, x):\n","        out = self.model(x)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"HejoSXe3vMVU"},"source":["# Define Model, Loss Function and Optimizer"]},{"cell_type":"markdown","metadata":{"id":"xAhGBH7-xxth"},"source":["Here we define the model, loss function, optimizer and optionally a learning rate scheduler."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:44.738652Z","iopub.status.busy":"2023-09-24T20:48:44.738351Z","iopub.status.idle":"2023-09-24T20:48:45.012783Z","shell.execute_reply":"2023-09-24T20:48:45.011830Z","shell.execute_reply.started":"2023-09-24T20:48:44.738628Z"},"id":"_qtrEM1ZvLje","outputId":"e1315af3-8285-44b5-ecf1-59dff317d99b","trusted":true},"outputs":[],"source":["INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\n","model       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\n","# summary(model, frames.to(device))\n","# Check number of parameters of your network\n","# Remember, you are limited to 20 million parameters for HW1 (including ensembles)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:45.014645Z","iopub.status.busy":"2023-09-24T20:48:45.014288Z","iopub.status.idle":"2023-09-24T20:48:45.020617Z","shell.execute_reply":"2023-09-24T20:48:45.019679Z","shell.execute_reply.started":"2023-09-24T20:48:45.014612Z"},"trusted":true},"outputs":[],"source":["def count_parameters(model=model):\n","    params = [p.numel() for p in model.parameters() if p.requires_grad]\n","    print(f'Model total parameters: {sum(params):>7}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:45.022900Z","iopub.status.busy":"2023-09-24T20:48:45.022206Z","iopub.status.idle":"2023-09-24T20:48:45.033250Z","shell.execute_reply":"2023-09-24T20:48:45.032266Z","shell.execute_reply.started":"2023-09-24T20:48:45.022867Z"},"trusted":true},"outputs":[],"source":["count_parameters()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:45.035199Z","iopub.status.busy":"2023-09-24T20:48:45.034817Z","iopub.status.idle":"2023-09-24T20:48:45.043687Z","shell.execute_reply":"2023-09-24T20:48:45.042811Z","shell.execute_reply.started":"2023-09-24T20:48:45.035164Z"},"id":"UROGEVJevKD-","trusted":true},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n","# We use CE because the task is multi-class classification\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr= config['init_lr']) #Defining Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay'])\n","\n","# Recommended : Define Scheduler for Learning Rate,\n","# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n","# You can refer to Pytorch documentation for more information on how to use them.\n","\n","# Is your training time very high?\n","# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n","# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html"]},{"cell_type":"markdown","metadata":{"id":"IBwunYpyugFg"},"source":["# Training and Validation Functions"]},{"cell_type":"markdown","metadata":{"id":"1JgeNhx4x2-P"},"source":["This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:45.045387Z","iopub.status.busy":"2023-09-24T20:48:45.045050Z","iopub.status.idle":"2023-09-24T20:48:45.204886Z","shell.execute_reply":"2023-09-24T20:48:45.204004Z","shell.execute_reply.started":"2023-09-24T20:48:45.045356Z"},"id":"XblOHEVtKab2","outputId":"7b3233dd-baf8-4c68-c235-b6805ffc1f59","trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:45.206997Z","iopub.status.busy":"2023-09-24T20:48:45.206187Z","iopub.status.idle":"2023-09-24T20:48:45.218050Z","shell.execute_reply":"2023-09-24T20:48:45.217057Z","shell.execute_reply.started":"2023-09-24T20:48:45.206964Z"},"id":"8wjPz7DHqKcL","trusted":true},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion):\n","\n","    model.train()\n","    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n","    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","\n","    for i, (frames, phonemes) in enumerate(dataloader):\n","\n","        ### Initialize Gradients\n","        optimizer.zero_grad()\n","\n","        ### Move Data to Device (Ideally GPU)\n","        frames      = frames.to(device)\n","        phonemes    = phonemes.to(device)\n","\n","        ### Forward Propagation\n","        logits  = model(frames)\n","\n","        ### Loss Calculation\n","        loss    = criterion(logits, phonemes)\n","\n","        ### Backward Propagation\n","        loss.backward()\n","\n","        ### Gradient Descent\n","        optimizer.step()\n","\n","        tloss   += loss.item()\n","        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n","\n","        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n","                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n","        batch_bar.update()\n","\n","        ### Release memory\n","        del frames, phonemes, logits\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    tloss   /= len(train_loader)\n","    tacc    /= len(train_loader)\n","\n","    return tloss, tacc"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:45.219820Z","iopub.status.busy":"2023-09-24T20:48:45.219352Z","iopub.status.idle":"2023-09-24T20:48:45.233940Z","shell.execute_reply":"2023-09-24T20:48:45.233039Z","shell.execute_reply.started":"2023-09-24T20:48:45.219750Z"},"id":"Q5npQNFH315V","trusted":true},"outputs":[],"source":["def eval(model, dataloader):\n","\n","    model.eval() # set model in evaluation mode\n","    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n","    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n","\n","    for i, (frames, phonemes) in enumerate(dataloader):\n","\n","        ### Move data to device (ideally GPU)\n","        frames      = frames.to(device)\n","        phonemes    = phonemes.to(device)\n","\n","        # makes sure that there are no gradients computed as we are not training the model now\n","        with torch.inference_mode():\n","            ### Forward Propagation\n","            logits  = model(frames)\n","            ### Loss Calculation\n","            loss    = criterion(logits, phonemes)\n","\n","        vloss   += loss.item()\n","        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n","\n","        # Do you think we need loss.backward() and optimizer.step() here?\n","\n","        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n","                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n","        batch_bar.update()\n","\n","        ### Release memory\n","        del frames, phonemes, logits\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    vloss   /= len(val_loader)\n","    vacc    /= len(val_loader)\n","\n","    return vloss, vacc"]},{"cell_type":"markdown","metadata":{"id":"yMd_XxPku5qp"},"source":["# Weights and Biases Setup"]},{"cell_type":"markdown","metadata":{"id":"tjIbhR1wwbgI"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:45.235647Z","iopub.status.busy":"2023-09-24T20:48:45.235124Z","iopub.status.idle":"2023-09-24T20:48:49.982956Z","shell.execute_reply":"2023-09-24T20:48:49.981955Z","shell.execute_reply.started":"2023-09-24T20:48:45.235615Z"},"id":"SCDYx5VEu6qI","outputId":"3c707dea-8715-4bc1-b660-e7add32e8d6e","trusted":true},"outputs":[],"source":["wandb.login(key=\"<wandb API key>\") #API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:49.985177Z","iopub.status.busy":"2023-09-24T20:48:49.984408Z","iopub.status.idle":"2023-09-24T20:48:51.313577Z","shell.execute_reply":"2023-09-24T20:48:51.312931Z","shell.execute_reply.started":"2023-09-24T20:48:49.985141Z"},"id":"xvUnYd3Bw2up","outputId":"a38e59c0-329a-4eb9-9676-e23429d7c9a8","trusted":true},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name    = \"second-run-Kaggle_5\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n","#     reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n","    id     = \"jkjzosux\", ### Insert specific run id here if you want to resume a previous run\n","    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw1p2\", ### Project should be created in your wandb account\n","    config  = config ### Wandb Config for your run\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:51.318314Z","iopub.status.busy":"2023-09-24T20:48:51.316643Z","iopub.status.idle":"2023-09-24T20:48:51.328903Z","shell.execute_reply":"2023-09-24T20:48:51.327521Z","shell.execute_reply.started":"2023-09-24T20:48:51.318286Z"},"id":"wft15E_IxYFi","outputId":"95c7d1c9-fcc5-4b10-f741-bd17e13c9d88","trusted":true},"outputs":[],"source":["# Save your model architecture as a string with str(model)\n","model_arch  = str(model)\n","\n","### Save it in a txt file\n","arch_file   = open(\"model_arch.txt\", \"w\")\n","file_write  = arch_file.write(model_arch)\n","arch_file.close()\n","\n","### log it in your wandb run with wandb.save()\n","wandb.save('model_arch.txt')"]},{"cell_type":"markdown","metadata":{"id":"nclx_04fu7Dd"},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{"id":"MdLMWfEpyGOB"},"source":["Now, it is time to finally run your ablations! Have fun!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T20:48:52.390379Z","iopub.status.busy":"2023-09-24T20:48:52.387848Z","iopub.status.idle":"2023-09-24T23:05:26.466916Z","shell.execute_reply":"2023-09-24T23:05:26.465537Z","shell.execute_reply.started":"2023-09-24T20:48:52.390331Z"},"id":"MG4F77Nm0Am9","trusted":true},"outputs":[],"source":["%%time\n","# Iterate over number of epochs to train and evaluate your model\n","torch.cuda.empty_cache()\n","gc.collect()\n","# wandb.watch(model, log=\"all\")\n","best_val_loss = float('inf')\n","best_val_acc = 0\n","for epoch in range(config['epochs']):\n","\n","    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n","\n","    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n","    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n","    val_loss, val_acc       = eval(model, val_loader)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_loss_HW1P2_2_5.pth')  # Save the best model\n","        wandb.save(\"best_loss_HW1P2_2_5.pth\")\n","        \n","    if val_acc > best_val_acc:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'best_acc_HW1P2_2_5.pth')  # Save the best model\n","        wandb.save(\"best_acc_HW1P2_2_5.pth\")\n","\n","    torch.save(model.state_dict(), 'HW1P2_2_5.pth')  # Save the current model\n","    wandb.save(\"HW1P2_2_5.pth\")\n","\n","    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n","    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n","\n","    ### Log metrics at each epoch in your run\n","    # Optionally, you can log at each batch inside train/eval functions\n","#     (explore wandb documentation/wandb recitation)\n","    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n","               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n","\n","    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n","\n","### Finish your wandb run\n","# run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T23:08:08.141923Z","iopub.status.busy":"2023-09-24T23:08:08.141475Z","iopub.status.idle":"2023-09-24T23:08:08.220242Z","shell.execute_reply":"2023-09-24T23:08:08.218186Z","shell.execute_reply.started":"2023-09-24T23:08:08.141889Z"},"id":"Cch4b3Bscuox","trusted":true},"outputs":[],"source":["model.load_state_dict(torch.load('best_acc_HW1P2_2_5.pth', map_location=torch.device(device)))"]},{"cell_type":"markdown","metadata":{"id":"_kXwf5YUo_4A"},"source":["# Testing and submission to Kaggle"]},{"cell_type":"markdown","metadata":{"id":"WI1hSFYLpJvH"},"source":["Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T23:08:09.143983Z","iopub.status.busy":"2023-09-24T23:08:09.143601Z","iopub.status.idle":"2023-09-24T23:08:09.153297Z","shell.execute_reply":"2023-09-24T23:08:09.151518Z","shell.execute_reply.started":"2023-09-24T23:08:09.143952Z"},"id":"R-SU9fZ3xHtk","trusted":true},"outputs":[],"source":["def test(model, test_loader):\n","    ### What you call for model to perform inference?\n","    model.eval # TODO train or eval?\n","\n","    ### List to store predicted phonemes of test data\n","    test_predictions = []\n","\n","    ### Which mode do you need to avoid gradients?\n","    with torch.no_grad(): # TODO\n","\n","        for i, mfccs in enumerate(tqdm(test_loader)):\n","\n","            mfccs   = mfccs.to(device)\n","\n","            logits  = model(mfccs)\n","\n","            ### Get most likely predicted phoneme with argmax\n","            predicted_phonemes = torch.argmax(logits, dim=1).cpu().numpy().flatten()\n","\n","            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n","            test_predictions.extend(predicted_phonemes)# TODO\n","\n","    return test_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T23:08:09.392750Z","iopub.status.busy":"2023-09-24T23:08:09.392453Z","iopub.status.idle":"2023-09-24T23:08:53.622204Z","shell.execute_reply":"2023-09-24T23:08:53.620684Z","shell.execute_reply.started":"2023-09-24T23:08:09.392725Z"},"id":"wG9v6Xmxu7wp","outputId":"de00f1e4-bfd0-47e8-9195-e173cc7860fa","trusted":true},"outputs":[],"source":["predictions = test(model, test_loader)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T23:08:53.630734Z","iopub.status.busy":"2023-09-24T23:08:53.627992Z","iopub.status.idle":"2023-09-24T23:08:53.640590Z","shell.execute_reply":"2023-09-24T23:08:53.639468Z","shell.execute_reply.started":"2023-09-24T23:08:53.630682Z"},"id":"jOSN1j5GuIpc","trusted":true},"outputs":[],"source":["PHONEMES_decoder = {i:p for i, p in enumerate(PHONEMES)}"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T23:08:53.643074Z","iopub.status.busy":"2023-09-24T23:08:53.642508Z","iopub.status.idle":"2023-09-24T23:08:54.089910Z","shell.execute_reply":"2023-09-24T23:08:54.088596Z","shell.execute_reply.started":"2023-09-24T23:08:53.643039Z"},"id":"4dWjcbHZtPyC","trusted":true},"outputs":[],"source":["predictions=[PHONEMES_decoder[target_value] for target_value in predictions]"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T23:08:54.094082Z","iopub.status.busy":"2023-09-24T23:08:54.092290Z","iopub.status.idle":"2023-09-24T23:08:56.378081Z","shell.execute_reply":"2023-09-24T23:08:56.376971Z","shell.execute_reply.started":"2023-09-24T23:08:54.094044Z"},"id":"ZE1hRnvf0bFz","trusted":true},"outputs":[],"source":["### Create CSV file with predictions\n","with open(\"./submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(predictions)):\n","        f.write(\"{},{}\\n\".format(i, predictions[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-24T23:08:56.379983Z","iopub.status.busy":"2023-09-24T23:08:56.379427Z","iopub.status.idle":"2023-09-24T23:09:01.920587Z","shell.execute_reply":"2023-09-24T23:09:01.919148Z","shell.execute_reply.started":"2023-09-24T23:08:56.379949Z"},"trusted":true},"outputs":[],"source":["# ### Submit to kaggle competition using kaggle API (Uncomment below to use)\n","!kaggle competitions submit -c 11785-hw1p2-f23 -f ./submission.csv -m \"Test Submission\"\n","\n","# ### However, its always safer to download the csv file and then upload to kaggle"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
