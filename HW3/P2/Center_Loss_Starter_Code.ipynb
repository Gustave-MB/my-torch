{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gustave-MB/my-torch/blob/main/HW3/P2/Center_Loss_Starter_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VdnO2As-Ild"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Center Loss"
      ],
      "metadata": {
        "id": "WCBe8x7C-gX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description:\n",
        "Briefly speaking, Center Loss will decrease the variation of the feature cluster of each class.\n",
        "\n",
        "In other words, the objective of Center Loss is to minimize the intra-class variance of the output feature (the output of the model before being passed to the final classification layer).\n",
        "\n",
        "$$\\mathcal{L}_C = \\frac{1}{2}\\sum_{i=1}^{m}||\\pmb{x}_i-\\pmb{c}_{y_i}||_2^2$$\n",
        "\n",
        "Here $\\mathcal{L}_C$ denotes Center Loss, $\\pmb{x}_i$ denotes the feature vector of class $i$, $\\pmb{c}_{y_i}$ denotes the center of feature vectors within the class of $y_i$, and $m$ is the number of ($\\pmb{x}_i$,$y_i$) pairs.\n",
        "\n",
        "What we will actually implement here will be the mean of the loss, so that the scale of loss matches with cross entropy loss.\n",
        "\n",
        "$$\\mathcal{L}_C = \\frac{1}{2m}\\sum_{i=1}^{m}||\\pmb{x}_i-\\pmb{c}_{y_i}||_2^2$$\n",
        "\n",
        "However, it is too time-wasting to calculate the intra-class centers of ALL the data in every epoch. Therefore, Wen et.al decides to update the centers by batches. \"In each iteration, the centers are computed by\n",
        "averaging the features of the corresponding classes (In this case, some of the\n",
        "centers may not update).\"\n",
        "\n",
        "The centers are updated by a learning rate $\\alpha$ .\n",
        "\n",
        "$$\\frac{\\partial\\mathcal{L}_C}{\\partial\\pmb{x}_i} = \\pmb{x}_i-\\pmb{c}_{y_i}$$\n",
        "\n",
        "$$\\Delta\\pmb{c}_j = \\frac{\\sum_{i=1}^{m}\\delta(y_i=j)\\cdot(\\pmb{c}_i-\\pmb{x}_i)}{1+\\sum_{i=1}^{m}\\delta(y_i=j)}$$\n",
        "\n",
        "$$\\pmb{c}_{j}^{t+1}=\\pmb{c}_{j}^{t}-\\alpha\\cdot\\Delta\\pmb{c}_j$$\n",
        "\n",
        "Inside the class of Center Loss, you do not need to implement the update part. Update is handled by the optimizer, which means that you only need to calculate the loss."
      ],
      "metadata": {
        "id": "AdIn5-KwPo9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"Center Loss\n",
        "        Center Loss Paper:\n",
        "        https://ydwen.github.io/papers/WenECCV16.pdf\n",
        "    Args:\n",
        "        nn (_type_): _description_\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_classes=NotImplemented, # TODO: What is the number of classes for our model?\n",
        "                 feat_dim=NotImplemented, # TODO: What is the dimension of your output feature?\n",
        "                 ) -> None:\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "\n",
        "        # I have written the initialization of centers for you here\n",
        "        # Consider why the shape of centers is (num_classes, feat_dim)\n",
        "        # You may want to adjust here if you want to test the program on cpu\n",
        "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (batch_size).\n",
        "        \"\"\"\n",
        "        centers = # TODO: Boradcast your self.centers so that centers[i] will contain the center of true label of x[i]\n",
        "        dist = # TODO: Calculate the squared euclidian distances between your inputs and current centers\n",
        "        # Each element in dist is actually the Center Loss of each input\n",
        "        dist = torch.clamp(dist, min=1e-12, max=1e+12)\n",
        "        # Here you have to first wrap 'dist' inside torch.clamp() function, because log(0) will cause NaN output.\n",
        "        # To avoid the 0 in 'dist', we will set the lower bound in 'dist' to a value that is close to 0\n",
        "\n",
        "        loss = # TODO: Calculate the mean loss across the batch.\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "w0jJGMYx-iPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example in Training Procedure"
      ],
      "metadata": {
        "id": "O9HaPj9V-iu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you use FP16 in your training, there is a specific usage you have to follow if you use multiple losses in your training. Here is the example code for multiple loss training when you use Center Loss\n",
        "\n",
        "More detailed information in this link:\n",
        "[link](https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-multiple-models-losses-and-optimizers)\n",
        "\n",
        "The hyperparameters you need to tune: loss weight $\\lambda$, loss learning rate $\\alpha$"
      ],
      "metadata": {
        "id": "bQd9-bb4_dvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "center_loss = CenterLoss(num_classes=NotImplemented, feat_dim=NotImplemented)\n",
        "optimizer_center_loss = torch.optim.SGD(center_loss.parameters(), lr = NotImplemented) # TODO: select a learning rate, I will recommend to use 0.1"
      ],
      "metadata": {
        "id": "P_4WrO7FHQrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module,\n",
        "          train_loader: Dataloader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          optimizer_center_loss: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          fine_tuning_loss: nn.Module, # here we are using Center Loss as our fine_tuning_loss\n",
        "          loss_weight,\n",
        "          scheduler: optim.lr_scheduler._LRScheduler,\n",
        "          scaler: torch.cuda.amp.GradScaler,\n",
        "          device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_center_loss.zero_grad()\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs, feats = model(images, return_feats=True)\n",
        "            loss0 = # TODO: calculate cross entropy loss from outputs and labels\n",
        "            loss1 = # TODO: calculate weighted fine_tuning_loss (center loss) from feats and labels\n",
        "\n",
        "        # TODO: backward loss0 to calculate gradients for model paramters\n",
        "        # Hint: You have to pass retain_graph=True here, so that the scaler will remember this backward call\n",
        "\n",
        "        # TODO: backward loss1 to calculate gradients for fine_tuning_loss paramters\n",
        "\n",
        "        # update fine tuning loss' parameters\n",
        "        # the paramerters should be adjusted according to the loss_weight you choose\n",
        "        for parameter in fine_tuning_loss.parameters():\n",
        "            parameter.grad.data *= (1.0 / loss_weight)\n",
        "\n",
        "        scaler.step(optimizer_center_loss)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "        # if you use a scheduler to schedule your learning rate for Center Loss\n",
        "        # scheduler_center_loss.step()\n",
        "\n",
        "        del images, labels, outputs, loss0, loss1\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "9iVa7KFC-m3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}