import json
from attention import Attention
import torch
import numpy as np
import traceback

np.set_printoptions(
    suppress=True,
    precision=4)

autograder_version = '3.0.1'
print("Autograder version: " + str(autograder_version))

input_dim = 5
seq_len = 10
batch_size = 1
output_dim = 5
d_q, d_k, d_v = 6, 6, 8

linear = torch.nn.Linear(d_v, output_dim)
criterion = torch.nn.CrossEntropyLoss()

TEST_attn_out = False
TEST_Q = False
TEST_K = False
TEST_V = False

TEST_dX = False
TEST_dV = False
TEST_dQ = False
TEST_dK = False

input = torch.tensor([[[-1.3079, -0.8491,  0.6997, -0.4443, -0.1539],
         [ 0.3456,  0.5846, -0.8788, -0.6073, -0.7499],
         [-1.9509, -0.1224,  0.5499, -0.0946, -1.1077],
         [-0.8996, -0.4065, -0.7383,  0.0861, -0.8568],
         [-0.1469, -0.4334, -0.7954,  1.6914, -0.2016],
         [ 0.8588,  1.5435,  0.9931, -1.5449,  1.0853],
         [ 0.3638, -0.4579,  0.2934, -0.5102,  2.3790],
         [-0.5550, -0.3282,  0.6595, -0.8302,  1.1644],
         [-0.6168, -1.3757, -0.9410,  1.2700, -1.6909],
         [-0.6403,  0.1126,  1.8095,  0.5627, -0.4171]]])

W_query = torch.tensor([[0.5548, 0.6673, 0.5811, 0.8343, 0.3551, 0.6755],
        [0.0552, 0.5737, 0.4620, 0.3588, 0.3220, 0.5404],
        [0.1047, 0.2338, 0.6783, 0.4242, 0.8953, 0.0045],
        [0.1726, 0.1560, 0.3676, 0.6872, 0.2900, 0.1641],
        [0.7342, 0.3356, 0.3969, 0.2146, 0.9328, 0.5411]])

W_key = torch.tensor([[0.7750, 0.1101, 0.5621, 0.0824, 0.9632, 0.2169],
        [0.5733, 0.1978, 0.2603, 0.9104, 0.8083, 0.1939],
        [0.4470, 0.1949, 0.8577, 0.4744, 0.7349, 0.8484],
        [0.4168, 0.0275, 0.7195, 0.3051, 0.9561, 0.7606],
        [0.3285, 0.9369, 0.0464, 0.4511, 0.0825, 0.0108]])

W_value = torch.tensor([[0.7042, 0.7897, 0.5744, 0.8326, 0.5845, 0.5800, 0.0099, 0.0989],
        [0.5530, 0.5623, 0.7372, 0.6123, 0.3681, 0.5035, 0.7252, 0.0773],
        [0.5831, 0.0368, 0.8959, 0.3983, 0.7856, 0.6458, 0.1438, 0.7437],
        [0.8000, 0.0565, 0.5283, 0.4072, 0.3924, 0.8402, 0.8192, 0.7122],
        [0.1916, 0.8920, 0.1566, 0.4289, 0.7428, 0.9047, 0.7933, 0.5267]])

att_out = torch.tensor([[[-1.1405, -2.1061, -1.4564, -1.6652, -1.8687, -1.8458, -1.1487,
          -0.8078],
        [-1.0764, -1.6202, -1.0861, -1.3961, -1.2779, -1.3982, -0.8565,
          -0.4943],
        [-1.1922, -2.2601, -1.5548, -1.7733, -1.9905, -1.9464, -1.2128,
          -0.8350],
        [-1.2184, -2.1590, -1.4729, -1.7350, -1.8724, -1.8842, -1.1574,
          -0.7939],
        [-0.6697, -0.8963, -0.5914, -0.8166, -0.6523, -0.7643, -0.4534,
          -0.2272],
        [ 0.9702,  1.2318,  1.6523,  1.2312,  1.6194,  1.3549,  0.7286,
          0.8381],
        [ 0.9581,  1.2867,  1.6291,  1.2497,  1.6120,  1.3502,  0.7344,
          0.7981],
        [ 0.0982, -0.0862,  0.2117,  0.0106,  0.1866,  0.1783,  0.1427,
          0.2686],
        [-1.2495, -2.1500, -1.4773, -1.7474, -1.8772, -1.9011, -1.1476,
          -0.8118],
        [ 0.4071,  0.4233,  0.7273,  0.4544,  0.7542,  0.6592,  0.3974,
          0.5280]]])

K = torch.tensor([[[-1.4234, -0.3320, -0.6829, -0.7538, -1.8694, -0.1943],
        [-0.2893, -0.7369, -0.8791, -0.3798, -0.4829, -1.0273],
        [-1.7396, -1.1722, -0.7763, -0.5399, -1.7558, -0.0643],
        [-1.5058, -1.1237, -1.2225, -1.1547, -1.7260, -0.8441],
        [-0.0791, -0.3993,  0.3300, -0.3589,  0.5242,  0.4936],
        [ 1.7070,  1.5677,  0.6751,  1.9653,  1.4171,  0.1648],
        [ 0.7194,  2.2215,  0.0802,  0.6698, -0.0956, -0.1233],
        [-0.2870,  1.0706, -0.3750,  0.2403, -1.0129, -0.2434],
        [-1.7135, -2.0727, -0.6766, -2.1250, -1.3229, -0.2512],
        [ 0.4747, -0.0709,  1.6069,  0.8917,  1.3077,  1.8416]]])

Q = torch.tensor([[[-0.8889, -1.3173, -0.9021, -1.4374, -0.3838, -1.4954],
        [-0.5234,  0.0141, -0.6461, -0.4530, -1.3514,  0.0400],
        [-1.8611, -1.6300, -1.2916, -1.7410, -1.3005, -1.9964],
        [-1.2130, -1.2802, -1.5198, -1.3343, -1.8856, -1.2802],
        [-0.0448, -0.3364, -0.2834,  0.5036, -0.6014, -0.1685],
        [ 1.1958,  1.8140,  1.7486,  0.8628,  2.2554,  1.7524],
        [ 1.8659,  0.7675,  0.9555,  0.4236,  2.3156,  1.2032],
        [ 0.4546, -0.1432,  0.1302, -0.6217,  1.1331, -0.0555],
        [-1.5389, -1.7902, -1.8365, -0.8975, -2.7134, -1.8709],
        [-0.3687,  0.0082,  0.9486,  0.5710,  1.2030, -0.4969]]])

V = torch.tensor([[[-1.3675, -1.6469, -1.0092, -1.5771, -0.8160, -1.2468, -1.0142,
          -0.0721],
        [-0.5753, -0.1339, -0.5961, -0.2733, -1.0685, -1.2614, -0.7914,
          -1.4017],
        [-1.4088, -2.5826, -0.9416, -1.9939, -1.6133, -1.9196, -0.9852,
          -0.4442],
        [-1.3841, -1.7256, -1.5665, -1.6244, -1.8581, -1.9060, -1.0190,
          -1.0594],
        [ 0.5076, -0.4732, -0.2545, -0.1022, -0.3563,  0.4216,  0.7955,
          0.4589],
        [ 1.0094,  2.4635,  1.8747,  1.8921,  2.0503,  1.6004,  0.8660,
          0.4142],
        [ 0.2217,  2.1339,  0.2373,  0.9520,  1.8415,  1.8935,  1.1830,
          1.1084],
        [-0.6288,  0.3932, -0.2261, -0.2390,  0.6120,  0.2947,  0.0949,
          0.4322],
        [-1.0518, -2.7318, -1.8054, -1.9388, -2.3638, -2.1208, -1.4401,
          -0.8533],
        [ 1.0367, -0.7160,  1.5683,  0.3068,  0.9997,  0.9493,  0.4656,
          1.4722]]])

dLXnew = torch.tensor([[[ 3.2446e-02, -1.1372e-02,  2.1730e-02,  1.1650e-03,  2.0282e-02,
          -2.4915e-02, -2.0948e-02,  5.1325e-03],
         [ 3.1896e-02, -1.0784e-02,  2.2721e-02,  2.4869e-03,  1.9939e-02,
          -2.3822e-02, -1.9624e-02,  5.6257e-03],
         [ 3.2496e-02, -1.1480e-02,  2.1460e-02,  8.5514e-04,  2.0370e-02,
          -2.5124e-02, -2.1221e-02,  4.9814e-03],
         [-1.3957e-02,  1.0949e-02, -4.1171e-05,  8.0661e-03, -1.8198e-02,
           1.3411e-02,  1.5488e-02,  8.5723e-03],
         [ 3.2981e-02, -3.6054e-02, -1.6346e-02, -1.6777e-02, -8.0315e-03,
          -6.8838e-03, -1.4778e-02, -5.0227e-03],
         [ 3.3581e-02, -3.5610e-02, -6.1872e-03, -6.1321e-03, -3.7285e-03,
          -2.8523e-04, -6.9763e-03, -4.0708e-03],
         [-7.7418e-03, -1.7845e-02, -4.9155e-03,  9.8954e-03,  3.0391e-02,
           2.8173e-02,  1.7781e-02, -3.1444e-02],
         [ 3.2789e-02, -1.0636e-02,  2.9358e-02,  9.4644e-03,  2.1330e-02,
          -1.9521e-02, -1.4411e-02,  7.1320e-03],
         [ 3.2161e-02, -1.1253e-02,  2.1468e-02,  1.0272e-03,  2.0182e-02,
          -2.4839e-02, -2.0912e-02,  5.0421e-03],
         [-6.9881e-03, -1.8740e-02, -8.2654e-03,  6.2727e-03,  2.8548e-02,
           2.5833e-02,  1.4988e-02, -3.1510e-02]]])

py_dK = torch.tensor([[[-0.0016, -0.0016, -0.0025,  0.0002, -0.0048, -0.0012],
         [-0.0015, -0.0004, -0.0019, -0.0008, -0.0040, -0.0004],
         [-0.0037, -0.0048, -0.0056,  0.0001, -0.0096, -0.0040],
         [ 0.0077,  0.0075,  0.0061,  0.0105,  0.0046,  0.0094],
         [-0.0010,  0.0006, -0.0004,  0.0003, -0.0022,  0.0004],
         [-0.0041, -0.0161, -0.0103, -0.0119, -0.0025, -0.0156],
         [-0.0012,  0.0003,  0.0013, -0.0023,  0.0026, -0.0011],
         [-0.0004,  0.0012,  0.0007, -0.0015,  0.0009,  0.0006],
         [-0.0041, -0.0047, -0.0049,  0.0020, -0.0098, -0.0042],
         [ 0.0098,  0.0180,  0.0175,  0.0033,  0.0247,  0.0161]]])


py_dV = torch.tensor([[[ 1.7451e-02, -9.2910e-03,  8.0017e-03, -1.0572e-04,  7.3233e-03,
          -9.7358e-03, -9.0098e-03,  1.6913e-03],
         [ 1.2101e-02, -8.0405e-03,  4.6520e-03, -4.8587e-06,  6.1823e-03,
          -5.2309e-03, -5.4795e-03, -5.8499e-04],
         [ 2.2009e-02, -1.1157e-02,  1.0684e-02,  7.4733e-05,  9.2889e-03,
          -1.2726e-02, -1.1538e-02,  2.5983e-03],
         [ 3.9801e-02, -1.5974e-02,  2.4646e-02,  2.4370e-03,  1.8634e-02,
          -2.6091e-02, -2.1880e-02,  7.6822e-03],
         [ 8.1433e-03, -7.4762e-03,  3.4721e-03,  1.4218e-03,  7.6717e-03,
          -9.5073e-04, -2.0304e-03, -3.3375e-03],
         [ 2.0469e-02, -4.0223e-02, -4.5441e-03,  4.7359e-03,  2.7236e-02,
           1.9805e-02,  7.2206e-03, -2.8838e-02],
         [ 6.9165e-03, -8.1909e-03,  9.6989e-04,  5.5798e-04,  6.0414e-03,
           7.5763e-04, -1.0411e-03, -4.2195e-03],
         [ 7.7004e-03, -6.4361e-03,  1.5565e-03, -5.1440e-04,  3.8374e-03,
          -2.3012e-03, -3.1023e-03, -1.4832e-03],
         [ 4.8762e-02, -1.7976e-02,  3.2646e-02,  4.0742e-03,  2.4759e-02,
          -3.3248e-02, -2.7321e-02,  1.0034e-02],
         [ 1.6310e-02, -2.8060e-02, -1.1015e-03,  3.6470e-03,  2.0110e-02,
           1.1748e-02,  3.5691e-03, -1.9103e-02]]])

py_dQ = torch.tensor([[[ 5.1157e-06, -7.0990e-04,  9.8335e-04, -2.2678e-04,  7.0524e-04,
           1.1204e-03],
         [ 1.2335e-03,  7.0261e-04,  1.8935e-03,  1.8619e-03,  1.8779e-03,
           1.9082e-03],
         [-2.8877e-04, -1.0731e-03,  1.0160e-03, -7.2111e-04,  5.5425e-04,
           1.2406e-03],
         [ 9.7259e-04,  2.1177e-03, -4.5814e-04,  1.8836e-03, -6.4545e-05,
          -7.6007e-04],
         [-4.0370e-02, -4.7343e-02, -1.8262e-02, -3.7099e-02, -3.2295e-02,
          -6.4305e-03],
         [-1.4437e-02, -2.1011e-02,  1.1629e-02, -1.2389e-02, -1.4801e-04,
           2.0496e-02],
         [ 5.9828e-03,  8.0615e-03, -3.5805e-03,  5.4718e-03,  8.6990e-04,
          -6.9578e-03],
         [ 2.3692e-02,  1.6832e-02,  2.1643e-02,  2.6339e-02,  2.8322e-02,
           1.7381e-02],
         [-3.1030e-04, -7.9631e-04,  1.1333e-03, -2.3979e-04,  4.5455e-04,
           1.4787e-03],
         [ 1.1349e-02,  1.2617e-02,  4.4988e-03,  1.0934e-02,  8.8571e-03,
           9.9438e-04]]])

py_dX = torch.tensor([[[ 0.0014, -0.0034,  0.0098, -0.0014, -0.0145],
         [ 0.0048, -0.0006,  0.0092,  0.0016, -0.0055],
         [-0.0054, -0.0098,  0.0042, -0.0098, -0.0223],
         [ 0.0482,  0.0351,  0.0736,  0.0420, -0.0086],
         [-0.1077, -0.0627, -0.0634, -0.0527, -0.0967],
         [-0.0207, -0.0137, -0.0163, -0.0078, -0.0293],
         [ 0.0117,  0.0031,  0.0104,  0.0102, -0.0004],
         [ 0.0833,  0.0479,  0.0642,  0.0474,  0.0669],
         [ 0.0226,  0.0063,  0.0444,  0.0067, -0.0327],
         [ 0.0857,  0.0662,  0.0947,  0.0893,  0.0479]]])

"""
────────────────────────────────────────────────────────────────────────────────────
# Instantiate Attention object
────────────────────────────────────────────────────────────────────────────────────
"""

try:
  usr_attn = Attention(W_key, W_query, W_value)
except Exception as exc:
  print (traceback.format_exc())
  print (exc)

"""
────────────────────────────────────────────────────────────────────────────────────
# Forward Pass
────────────────────────────────────────────────────────────────────────────────────
"""

try:
  usr_out = usr_attn.forward(input)

  print("──────────────────────────────────────────")
  print("Attention outputs | STUDENT OUTPUT")
  print("──────────────────────────────────────────")

  print("\nAttention Output =\n", usr_out, sep="")
  print("\nK =\n", usr_attn.K, sep="")
  print("\nQ =\n", usr_attn.Q, sep="")
  print("\nV =\n", usr_attn.V, sep="")


  print("\n──────────────────────────────────────────")
  print("Attention outputs | SOLUTION OUTPUT")
  print("──────────────────────────────────────────")

  print("\nAttention output =\n", att_out, sep="")
  print("\nK =\n", K, sep="")
  print("\nQ =\n", Q, sep="")
  print("\nV =\n", V, sep="")


  print("\n──────────────────────────────────────────")
  print("Forward Pass | TEST RESULTS")
  print("──────────────────────────────────────────")

  print("\n           Pass?")

  TEST_attn_out = torch.allclose(usr_out, att_out, atol=1e-4)
  print("Test attention output:   ", TEST_attn_out)

  TEST_K = torch.allclose(K, usr_attn.K, atol=1e-4)
  print("Test K:   ", TEST_K)

  TEST_Q = torch.allclose(Q, usr_attn.Q, atol=1e-4)
  print("Test Q:   ", TEST_Q)

  TEST_V = torch.allclose(V, usr_attn.V, atol=1e-4)
  print("Test V:   ", TEST_V)

except Exception as exc:
  print (traceback.format_exc())
  print (exc)


"""
────────────────────────────────────────────────────────────────────────────────────
# Backward Pass
────────────────────────────────────────────────────────────────────────────────────
"""

try:
  usr_dLdX = usr_attn.backward(dLXnew)

  print("──────────────────────────────────────────")
  print("Backward Pass | STUDENT OUTPUT")
  print("──────────────────────────────────────────")

  print("\ndLdK =\n", usr_attn.dLdK, sep="")
  print("\ndLdQ =\n", usr_attn.dLdQ, sep="")
  print("\ndLdV =\n", usr_attn.dLdV, sep="")
  print("\ndLdX =\n", usr_dLdX, sep="")


  print("\n──────────────────────────────────────────")
  print("Backward Pass | SOLUTION OUTPUT")
  print("──────────────────────────────────────────")

  print("\ndLdK =\n", py_dK, sep="")
  print("\ndLdQ =\n", py_dQ, sep="")
  print("\ndLdV =\n", py_dV, sep="")
  print("\ndLdX =\n", py_dX, sep="")
  
  print("\n──────────────────────────────────────────")
  print("Backward Pass | TEST RESULTS")
  print("──────────────────────────────────────────")

  print("           Pass?")

  TEST_dK = torch.allclose(usr_attn.dLdK, py_dK, atol=1e-4)
  print("Test dLdK:   ", TEST_dK)
  TEST_dQ = torch.allclose(usr_attn.dLdQ, py_dQ, atol=1e-4)
  print("Test dLdQ:   ", TEST_dQ)
  TEST_dV = torch.allclose(usr_attn.dLdV, py_dV, atol=1e-4)
  print("Test dLdV:   ", TEST_dV)
  TEST_dX = torch.allclose(usr_dLdX, py_dX, atol=1e-4)
  print("Test dLdX:   ", TEST_dX)

except Exception as exc:
  print (traceback.format_exc())
  print (exc)


"""
────────────────────────────────────────────────────────────────────────────────────
## SCORE AND GRADE TESTS
────────────────────────────────────────────────────────────────────────────────────
"""

TEST_forward = (
    TEST_attn_out and
    TEST_Q and
    TEST_K and
    TEST_V
)

TEST_backward = (
    TEST_dX and
    TEST_dV and
    TEST_dQ and
    TEST_dK)


SCORE_LOGS = {
    "Forward": 10 * int(TEST_forward),
    "Backward": 10 * int(TEST_backward)
}


print("\n")
print("TEST   | STATUS | POINTS | DESCRIPTION")
print("───────┼────────┼────────┼────────────────────────────────")

for i, (key, value) in enumerate(SCORE_LOGS.items()):

    index_str = str(i).zfill(1)
    point_str = str(value).zfill(2) + "     │ "

    if value == 0:
        status_str = " │ FAILED │ "
    else:
        status_str = " │ PASSED │ "

    print("Test ", index_str, status_str, point_str, key, sep="")

print("\n")

"""
────────────────────────────────────────────────────────────────────────────────────
## FINAL SCORES
────────────────────────────────────────────────────────────────────────────────────
"""

print(json.dumps({'scores': SCORE_LOGS}))